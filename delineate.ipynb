{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "\n",
    "import graph_delineator.delineate as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MERIT data directories\n",
    "merit_dirs = {\n",
    "    'basins': Path('/nas/cee-ice/data/MERIT'),\n",
    "    'flow_dir': Path('/nas/cee-ice/data/MERIT-Hydro/processed/dir/'),\n",
    "    'flow_acc': Path('/nas/cee-ice/data/MERIT-Hydro/processed/upg/')\n",
    "}\n",
    "gauges_csv = Path('/nas/cee-water/cjgleason/ted/graph_delineator/data/clamped_manual_matchups.csv')\n",
    "output_dir = Path('/nas/cee-water/cjgleason/ted/graph_delineator/data/clamped_manual_matchups_500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.delineate_basins(gauges_csv, merit_dirs, 500, save_plots=True, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "basin_file_dir = output_dir / 'subbasins'\n",
    "subs_list = []\n",
    "for f in tqdm(basin_file_dir.glob(\"*.parquet\")):\n",
    "    outlet_id = f.stem.split('_subbasins')[0]\n",
    "    sub_gdf = gpd.read_parquet(f).rename(columns={'id':'site_id'})\n",
    "    sub_gdf['outlet_id'] = outlet_id\n",
    "    subs_list.append(sub_gdf)\n",
    "subbasins = pd.concat(subs_list).set_index('site_id')\n",
    "\n",
    "subbasins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gdf = gpd.read_parquet('/nas/cee-water/cjgleason/ted/swot-ml/data/multigraph_manual/metadata/subbasins_500.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subbasins = gdf\n",
    "subbasins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'USGS-07374000'\n",
    "subbasins = pd.concat([\n",
    "    subbasins[subbasins[\"outlet_id\"] == target], \n",
    "    subbasins[subbasins[\"outlet_id\"] != target]\n",
    "])\n",
    "subbasins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import BasinDataLake\n",
    "\n",
    "store = BasinDataLake(args.save_dir)\n",
    "status = store.get_processing_status(source='era5')\n",
    "processed_basins = [] if status.empty else status.index.get_level_values('basin').unique()\n",
    "to_process = subbasins[subbasins['outlet_id'].isin(processed_basins)]\n",
    "\n",
    "to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subbasins.to_parquet(\"/nas/cee-water/cjgleason/ted/swot-ml/data/multigraph_manual/metadata/subbasins_500.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"UKEA-694039\" # single gauge with weird location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gauges = gd.load_gauges(gauges_csv)\n",
    "megabasins_outlets = gd.get_pfaf1_outlet_dict(all_gauges)\n",
    "\n",
    "b = False\n",
    "for megabasin_id, outlet_dict in megabasins_outlets.items():\n",
    "    for outlet_id, outlet_gauge_ids in outlet_dict.items():   \n",
    "\n",
    "        if outlet_id != 'EAUF-E5300213':\n",
    "            continue\n",
    "            \n",
    "        print(outlet_id)\n",
    "        pfaf1_catchments, pfaf1_rivers = gd.load_basin_data(megabasin_id, merit_dirs)\n",
    "        \n",
    "        gauges = all_gauges[all_gauges[\"id\"].isin(outlet_gauge_ids)]\n",
    "        gauges = gd.set_gauge_area_range(gauges, pfaf1_rivers)\n",
    "\n",
    "        final_comid = gauges[gauges['id'] == outlet_id]['COMID'].item()\n",
    "        comids = gd.get_network_comids(final_comid,  pfaf1_rivers)\n",
    "\n",
    "        catchments = pfaf1_catchments.loc[pfaf1_catchments.index.isin(comids)]\n",
    "        rivers = pfaf1_rivers.loc[pfaf1_rivers.index.isin(comids)]\n",
    "        \n",
    "        G = gd.build_graph_with_geometries(catchments, rivers)\n",
    "\n",
    "        b = True\n",
    "        break\n",
    "    \n",
    "    if b:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchments.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_info = {}\n",
    "node_replacements = {}\n",
    "\n",
    "gauges['node_id'] = gauges['COMID'].astype(str)\n",
    "gauges = gauges.sort_values(['node_id', 'position'], ascending=False)\n",
    "gauges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_delineator.split_catchment import split_catchment_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merit_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfaf2_id = 23\n",
    "MERIT_RES = 0.000833333  # 3 arc second resolution in degrees\n",
    "TINY_AREA_THRESHOLD_KM2 = 0.5 \n",
    "\n",
    "for _, gauge in gauges.iterrows():\n",
    "    gauge_id = str(gauge[\"id\"])\n",
    "\n",
    "    # Follow the replacement chain to find current node\n",
    "    original_node_id = gauge[\"node_id\"]\n",
    "    current_node_id = original_node_id\n",
    "    while current_node_id in node_replacements:\n",
    "        current_node_id = node_replacements[current_node_id]\n",
    "\n",
    "    # Get the current polygon (may have been split already)\n",
    "    current_polygon = G.nodes[current_node_id][\"polygon\"]\n",
    "\n",
    "    gauge_polygon, is_leaf = split_catchment_raster(\n",
    "        gauge=gauge.to_dict(),\n",
    "        basin=pfaf2_id,\n",
    "        catchment_poly=current_polygon,\n",
    "        flow_dir_path=merit_dirs['flow_dir'],\n",
    "        flow_acc_path=merit_dirs['flow_acc'],\n",
    "    )\n",
    "\n",
    "    is_leaf = False\n",
    "\n",
    "    \n",
    "    if gauge_polygon is None:\n",
    "        # Failed, reason will be logged by fn\n",
    "        continue\n",
    "\n",
    "    if gauge_polygon == current_polygon:\n",
    "        # No split needed. Convert the node to a gauge\n",
    "        gd.convert_node_to_gauge(G, gauge_id, gauge[\"lat\"], gauge[\"lng\"], current_node_id)\n",
    "        # Track that this node was replaced\n",
    "        node_replacements[current_node_id] = gauge_id\n",
    "        gauge_info[gauge_id] = {\n",
    "            \"original_node\": original_node_id, \n",
    "            \"current_node\": current_node_id, \n",
    "            \"method\": \"replace\",\n",
    "        }\n",
    "        continue\n",
    "\n",
    "    # Calculate remainder\n",
    "    remainder_polygon = current_polygon.difference(gauge_polygon)\n",
    "    remainder_polygon = remainder_polygon.buffer(-MERIT_RES / 2).buffer(MERIT_RES / 2)\n",
    "\n",
    "    if is_leaf:\n",
    "        gd.add_leaf_gauge_to_graph(\n",
    "            G,\n",
    "            gauge_id,\n",
    "            gauge['lat'],\n",
    "            gauge['lng'],\n",
    "            gauge_polygon,\n",
    "            remainder_polygon,\n",
    "            current_node_id\n",
    "        )\n",
    "    else:\n",
    "        gd.insert_gauge_into_graph(\n",
    "            G,\n",
    "            gauge_id,\n",
    "            gauge[\"lat\"],\n",
    "            gauge[\"lng\"],\n",
    "            gauge_polygon,\n",
    "            remainder_polygon,\n",
    "            current_node_id,\n",
    "        )\n",
    "        gauge_info[gauge_id] = {\"original_node\": original_node_id, \"current_node\": current_node_id, \"method\": \"split\"}\n",
    "\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs, g = gd.DelineationResult(G, '').to_geodataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_delineator.split_catchment import split_catchment_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_backup = G.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = G_backup.copy()\n",
    "gauges_gdf = gauges\n",
    "flow_dir_path = merit_dirs['flow_dir']\n",
    "basin_id = megabasin_id\n",
    "\n",
    "gauge_info = {}\n",
    "# Track node replacements: old_node_id -> new_gauge_id\n",
    "node_replacements = {}\n",
    "\n",
    "# Find which catchment each gauge falls in (INITIAL spatial join only)\n",
    "catchment_polys = []\n",
    "node_ids = []\n",
    "for node_id, data in G.nodes(data=True):\n",
    "    if data.get(\"polygon\"):\n",
    "        catchment_polys.append(data[\"polygon\"])\n",
    "        node_ids.append(node_id)\n",
    "\n",
    "catchments_gdf = gpd.GeoDataFrame(\n",
    "    {\"node_id\": node_ids}, geometry=catchment_polys, crs=gauges_gdf.crs\n",
    ")\n",
    "\n",
    "ax = catchments_gdf.plot('node_id', edgecolor='black', linewidth=0.8)\n",
    "gauges.plot(color='black', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauges_with_nodes = gpd.sjoin(\n",
    "    gauges_gdf, catchments_gdf, how=\"left\", predicate=\"within\"\n",
    ")\n",
    "# Process from upstream (0.0) to downstream (1.0)\n",
    "gauges_with_nodes = gauges_with_nodes.sort_values(['node_id', 'position'])\n",
    "gauge_iter = gauges_with_nodes.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, gauge = next(gauge_iter)\n",
    "gauge_id = gauge[\"id\"]\n",
    "river_position = gauge.get(\"position\")\n",
    "\n",
    "# Follow the replacement chain to find current node\n",
    "original_node_id = gauge[\"node_id\"]\n",
    "current_node_id = original_node_id\n",
    "while current_node_id in node_replacements:\n",
    "    current_node_id = node_replacements[current_node_id]\n",
    "\n",
    "# Get the current polygon (may have been split already)\n",
    "current_polygon = G.nodes[current_node_id][\"polygon\"]\n",
    "current_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_poly = current_polygon\n",
    "bounds = catchment_poly.bounds\n",
    "bounds_list = [float(i) for i in bounds]\n",
    "\n",
    "bounds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ceil, floor\n",
    "MERIT_RES = 0.000833333  # 3 arc second resolution in degrees\n",
    "halfpix = MERIT_RES / 2  # Half pixel width\n",
    "\n",
    "# Adjust bounds to pixel centers\n",
    "bounds_list[0] = floor(bounds_list[0] * 1200) / 1200 - halfpix\n",
    "bounds_list[1] = floor(bounds_list[1] * 1200) / 1200 - halfpix\n",
    "bounds_list[2] = ceil(bounds_list[2] * 1200) / 1200 + halfpix\n",
    "bounds_list[3] = ceil(bounds_list[3] * 1200) / 1200 + halfpix\n",
    "bounding_box = tuple(bounds_list)\n",
    "bounds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megabasin_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pysheds.grid import Grid\n",
    "\n",
    "pfaf2_id = int(final_comid//1E6)\n",
    "\n",
    "flow_file = str(flow_dir_path / f\"{pfaf2_id}.tif\")\n",
    "\n",
    "grid = Grid.from_raster(flow_file, window=bounding_box, nodata=0)\n",
    "fdir = grid.read_raster(flow_file, window=bounding_box, nodata=0)\n",
    "\n",
    "facc_file = str(merit_dirs['flow_acc'] / f\"{pfaf2_id}.tif\")\n",
    "facc = grid.read_raster(facc_file, window=bounding_box, nodata=0)\n",
    "\n",
    "\n",
    "mymask = grid.rasterize([catchment_poly])\n",
    "\n",
    "fdir[mymask == 0] = 0\n",
    "facc[mymask == 0] = 0\n",
    "dirmap = (64, 128, 1, 2, 4, 8, 16, 32)  # ESRI flow direction standard\n",
    "\n",
    "plt.imshow(fdir)\n",
    "plt.show()\n",
    "plt.imshow(np.log10(facc+0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_pixel_area_m2(gauge, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pixels = min_area * 1E6 / pixel_area_m2\n",
    "min_pixels\n",
    "\n",
    "x_snap, y_snap = grid.snap_to_mask(facc > min_pixels, (gauge['lng'], gauge['lat']))\n",
    "\n",
    "x_snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = grid.catchment(\n",
    "    fdir=fdir,\n",
    "    x=x_snap, \n",
    "    y=y_snap,\n",
    "    dirmap=dirmap,\n",
    "    xytype=\"coordinate\",\n",
    "    recursionlimit=15000,\n",
    ")\n",
    "\n",
    "plt.imshow(catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.clip_to(catch)\n",
    "clipped_catch = grid.view(catch, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(facc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(facc > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facc * catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10((facc * catch)+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log10((facc * catch)+0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as ndi\n",
    "\n",
    "# 2. Erode the mask. This \"shrinks\" it by one pixel.\n",
    "#    Pixels that were on the edge are set to False.\n",
    "eroded_mask = ndi.binary_erosion(catch)\n",
    "\n",
    "# 3. Find the edge by selecting pixels that are in the\n",
    "#    original mask but *not* in the eroded mask.\n",
    "catch_edge = catch & ~eroded_mask\n",
    "\n",
    "# 4. Find pixels that are on the catchment edge AND have acc > 1\n",
    "outlets_on_edge = (facc > 100) | catch_edge*2\n",
    "\n",
    "# 5. Count them\n",
    "num_outlets_on_edge = np.sum(outlets_on_edge)\n",
    "print(num_outlets_on_edge)\n",
    "\n",
    "plt.imshow(outlets_on_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(catch_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(outlets_on_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create the edge mask (as you did)\n",
    "eroded_mask = ndi.binary_erosion(catch)\n",
    "catch_edge = catch & ~eroded_mask\n",
    "\n",
    "# 2. STRICT FILTER: Intersection of Edge AND High Accumulation\n",
    "# Use '&' not '|'. Adjust threshold (100) based on your resolution.\n",
    "significant_edge_flow = catch_edge & (facc >= (min_pixels*0.95))\n",
    "\n",
    "# 3. Label the clusters (Connected Components)\n",
    "# structure=np.ones((3,3)) allows diagonal connections (8-connectivity)\n",
    "labeled_array, num_features = ndi.label(significant_edge_flow, structure=np.ones((3,3)))\n",
    "\n",
    "print(f\"Number of flow locations found: {num_features}\")\n",
    "\n",
    "# Visual check\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(catch)\n",
    "plt.title(\"Catchment\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(labeled_array, cmap='nipy_spectral', interpolation='nearest')\n",
    "plt.title(f\"Identified Clusters: {num_features}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define ESRI directional offsets: (dy, dx)\n",
    "# 64:N, 128:NE, 1:E, 2:SE, 4:S, 8:SW, 16:W, 32:NW\n",
    "esri_offsets = {\n",
    "    64:  (-1, 0),  128: (-1, 1),\n",
    "    1:   (0, 1),   2:   (1, 1),\n",
    "    4:   (1, 0),   8:   (1, -1),\n",
    "    16:  (0, -1),  32:  (-1, -1)\n",
    "}\n",
    "\n",
    "# 1. Erode to find edges\n",
    "eroded_mask = ndi.binary_erosion(catch)\n",
    "catch_edge = catch & ~eroded_mask\n",
    "\n",
    "# 2. Filter: Edge pixels AND High Accumulation (Fixing the bitwise operator)\n",
    "# Adjust '100' based on your specific resolution/stream threshold requirements\n",
    "significant_edge_flow = catch_edge & (facc > 100)\n",
    "\n",
    "# 3. Label clusters\n",
    "labeled_array, num_features = ndi.label(significant_edge_flow, structure=np.ones((3,3)))\n",
    "\n",
    "print(f\"Found {num_features} flow connections on the edge.\")\n",
    "\n",
    "inflow_count = 0\n",
    "outflow_count = 0\n",
    "\n",
    "# 4. Iterate through each cluster to classify\n",
    "for i in range(1, num_features + 1):\n",
    "    # Get coordinates of all pixels in this cluster\n",
    "    y_locs, x_locs = np.where(labeled_array == i)\n",
    "    \n",
    "    # Find the pixel with the MAXIMUM flow accumulation in this cluster\n",
    "    # This is the most reliable point to check (center of the stream)\n",
    "    cluster_faccs = facc[y_locs, x_locs]\n",
    "    max_idx = np.argmax(cluster_faccs)\n",
    "    \n",
    "    py, px = y_locs[max_idx], x_locs[max_idx]\n",
    "    direction = fdir[py, px]\n",
    "    \n",
    "    # Get the offset for this direction\n",
    "    dy, dx = esri_offsets.get(direction, (0, 0))\n",
    "    \n",
    "    if dy == 0 and dx == 0:\n",
    "        print(f\"Cluster {i}: Sink or Invalid Direction (Val: {direction})\")\n",
    "        continue\n",
    "\n",
    "    # Calculate target coordinates\n",
    "    ny, nx = py + dy, px + dx\n",
    "    \n",
    "    # Check bounds and mask status\n",
    "    h, w = catch.shape\n",
    "    if 0 <= ny < h and 0 <= nx < w:\n",
    "        target_in_catchment = catch[ny, nx]\n",
    "    else:\n",
    "        # If it flows off the grid, it's definitely leaving the catchment\n",
    "        target_in_catchment = False\n",
    "\n",
    "    # CLASSIFICATION LOGIC\n",
    "    if target_in_catchment:\n",
    "        # If the edge pixel flows INTO the mask, it is an INFLOW\n",
    "        # (Water arrived here from outside and is continuing in)\n",
    "        print(f\"Cluster {i}: INFLOW (Max Acc: {cluster_faccs[max_idx]})\")\n",
    "        inflow_count += 1\n",
    "    else:\n",
    "        # If the edge pixel flows OUT of the mask, it is an OUTFLOW\n",
    "        print(f\"Cluster {i}: OUTFLOW (Max Acc: {cluster_faccs[max_idx]})\")\n",
    "        outflow_count += 1\n",
    "\n",
    "print(f\"Summary: {inflow_count} Inflows, {outflow_count} Outflows\")\n",
    "\n",
    "# Visual check\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(catch)\n",
    "plt.title(\"Catchment\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(labeled_array, cmap='nipy_spectral', interpolation='nearest')\n",
    "plt.title(f\"Identified Clusters: {num_features}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Adjust your figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate through the clusters found in the previous step\n",
    "for i in range(1, num_features + 1):\n",
    "    # 1. Find bounds of this cluster to zoom in\n",
    "    y_c, x_c = np.where(labeled_array == i)\n",
    "    \n",
    "    # Add padding to view context\n",
    "    pad = 5\n",
    "    y_min, y_max = np.min(y_c) - pad, np.max(y_c) + pad\n",
    "    x_min, x_max = np.min(x_c) - pad, np.max(x_c) + pad\n",
    "    \n",
    "    # 2. Identify the Critical Pixel (Max Flow Acc)\n",
    "    cluster_faccs = facc[y_c, x_c]\n",
    "    max_idx = np.argmax(cluster_faccs)\n",
    "    py, px = y_c[max_idx], x_c[max_idx]\n",
    "    \n",
    "    # 3. Get Flow Vector\n",
    "    direction = fdir[py, px]\n",
    "    dy, dx = esri_offsets.get(direction, (0,0))\n",
    "    \n",
    "    # Setup Subplot\n",
    "    ax = plt.subplot(1, num_features, i)\n",
    "    \n",
    "    # Plot the Catchment Mask (Yellow=In, Purple=Out)\n",
    "    subset_mask = catch[y_min:y_max, x_min:x_max]\n",
    "    ax.imshow(subset_mask, cmap='viridis', alpha=0.6, origin='upper', extent=[x_min, x_max, y_max, y_min])\n",
    "    \n",
    "    # Plot the Cluster Pixels (Red)\n",
    "    ax.scatter(x_c, y_c, c='red', s=10, label='Cluster')\n",
    "    \n",
    "    # Plot the Flow Arrow from the Max Facc Pixel\n",
    "    # Note: Quiver (X, Y, U, V) -> U corresponds to dx (cols), V to -dy (rows, because image y is down)\n",
    "    ax.quiver(px + 0.5, py + 0.5, dx, -dy, color='white', scale=1, scale_units='xy', angles='xy')\n",
    "    \n",
    "    # Highlight the target pixel\n",
    "    target_y, target_x = py + dy, px + dx\n",
    "    is_in = catch[target_y, target_x]\n",
    "    status = \"IN (Inflow)\" if is_in else \"OUT (Outflow)\"\n",
    "    f\n",
    "    ax.set_title(f\"Cluster {i}\\nTarget is {status}\")\n",
    "    ax.invert_yaxis() # Align with image coordinates\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.unique(fdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkb\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "\n",
    "mymask = grid.rasterize([catchment_poly])\n",
    "\n",
    "fdir = grid.read_raster(flow_file, window=bounding_box, nodata=0)\n",
    "m, n = grid.shape\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        if int(mymask[i, j]) == 0:\n",
    "            fdir[i, j] = 0\n",
    "\n",
    "plt.imshow(fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = grid.catchment(\n",
    "    fdir=fdir,\n",
    "    x=gauge['lng'],\n",
    "    y=gauge['lat'],\n",
    "    dirmap=dirmap,\n",
    "    xytype=\"coordinate\",\n",
    "    recursionlimit=15000,\n",
    ")\n",
    "plt.imshow(catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_polygon = split_catchment_raster(\n",
    "    gauge_id=gauge_id,\n",
    "    basin=basin_id,\n",
    "    lat=gauge[\"lat\"],\n",
    "    lng=gauge[\"lng\"],\n",
    "    river_position=river_position,\n",
    "    catchment_poly=current_polygon,\n",
    "    flow_dir_path=flow_dir_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERIT_RES = 0.000833333  # 3 arc second resolution in degrees\n",
    "\n",
    "remainder_polygon = current_polygon.difference(gauge_polygon)\n",
    "remainder_polygon = remainder_polygon.buffer(-MERIT_RES / 2).buffer(MERIT_RES / 2)\n",
    "\n",
    "remainder_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERIT_RES * MERIT_RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_polygon.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subbasins, gauges = results['USGS-01059000'].to_geodataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = subbasins[subbasins['is_gauge']].plot('id', alpha=0.8, cmap='inferno')\n",
    "subbasins.plot(color='grey', ax=ax, zorder=0)\n",
    "gauges.plot('id', ax=ax, cmap='inferno')#, style_kwds={'markeredgecolor':'black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"/nas/cee-water/cjgleason/ted/graph_delineator/outputs/test\")\n",
    "gd.save_results(results, output_dir, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-delineator",
   "language": "python",
   "name": "graph-delineator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
